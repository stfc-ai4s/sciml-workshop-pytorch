{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DNN: backprop\n",
    "\n",
    "Deep learning applications have been tremendously facilitated by various open-source libraries such as [TensorFlow](https://www.tensorflow.org/), [PyTorch](https://pytorch.org/) and [Apache MXNet](https://mxnet.apache.org/). This notebook aims at delivering a deeper understanding of what happens inside the black box by \n",
    "\n",
    "* demonstrating Gradient Descent in 1D;\n",
    "* demonstrating different levels of implementing the training loop with Pytorch;\n",
    "* creating a fully-connected DNN from scratch based on Pytorch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install dependencies\n",
    "!pip install tqdm\n",
    "!pip install matplotlib\n",
    "!pip install numpy\n",
    "# You can install any Pytorch version, but recommend to install >=1.0.0\n",
    "!pip install torch==2.2.0 torchvision==0.17.0 torchaudio==2.2.0 --index-url https://download.pytorch.org/whl/cu118"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from tqdm import trange\n",
    "import matplotlib.pyplot as plt\n",
    "# plt.style.use('ggplot')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gradient Descent in 1D\n",
    "\n",
    "Gradient Descent is a first-order iterative approach to solve a non-linear programming problem with continuous differentiability. To better understand it, let us solve a 1D problem:\n",
    "\n",
    "> $\\underset{x}{\\mathrm{argmin}}\\ f(x)$\n",
    "\n",
    "The alogrithm is\n",
    "\n",
    "> $x_{k+1} = x_{k}-\\eta \\left . \\dfrac{\\partial f}{\\partial x} \\right\\vert _{x=x_k},\\quad \\eta\\in R^+$\n",
    "\n",
    "The algorithm can be simply implemented as"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def GD_1D(f, df, x0, eta, n_iter, alpha=0.):\n",
    "    \"\"\"\n",
    "    GD in 1D\n",
    "    :param f: target function\n",
    "    :param df: derivative of target function\n",
    "    :param x0: inital guess\n",
    "    :param eta: learning rate\n",
    "    :param n_iter: number of interations\n",
    "    :param alpha: momentum decay factor  (default=0., no momentum)\n",
    "    :return: trajactory of convergence\n",
    "    \"\"\"\n",
    "    # allocate arrays\n",
    "    x = np.zeros((n_iter + 1))\n",
    "    y = np.zeros((n_iter + 1))\n",
    "    \n",
    "    # initial guess\n",
    "    x[0] = x0\n",
    "    y[0] = f(x0)\n",
    "    \n",
    "    # iteration\n",
    "    dx = 0.\n",
    "    for k in range(n_iter):\n",
    "        dx = - eta * df(x[k]) + alpha * dx\n",
    "        x[k + 1] = x[k] + dx\n",
    "        y[k + 1] = f(x[k + 1])\n",
    "        \n",
    "    # return trajactory\n",
    "    return x, y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we can choose a non-linear function to try out GD:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# f(x)\n",
    "def f(x):\n",
    "    return 3 * x ** 4 - 6 * x ** 2 - 2 * x + 2\n",
    "\n",
    "# ∂f/∂x\n",
    "def df(x):\n",
    "    return 12 * x ** 3 - 12 * x - 2\n",
    "    \n",
    "# initial guess\n",
    "x0 = 0\n",
    "\n",
    "# learning rate\n",
    "eta = 0.01\n",
    "\n",
    "# num iter\n",
    "n_iter = 100\n",
    "\n",
    "# perform GD\n",
    "x, y = GD_1D(f, df, x0, eta, n_iter, alpha=0.)\n",
    "\n",
    "# display results\n",
    "print(f'final x: {x[-1]}')\n",
    "print(f'final y: {y[-1]}')\n",
    "\n",
    "# plot range\n",
    "x_range = np.linspace(-1.5, 1.5, 1000)\n",
    "plt.figure(dpi=200)\n",
    "plt.plot(x_range, f(x_range), ls='--', label='$f(x)$')\n",
    "plt.plot(x, y, label='Trajectory', marker='.')\n",
    "plt.scatter(x[0], y[0], c='b', zorder=100, s=100, marker='+')\n",
    "plt.scatter(x[-1], y[-1], c='r', zorder=100, s=100, marker='+')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercises\n",
    "\n",
    "Implement [AdaGrad](https://en.wikipedia.org/wiki/Stochastic_gradient_descent#AdaGrad), [RMSProp](https://en.wikipedia.org/wiki/Stochastic_gradient_descent#RMSProp) and [Adam](https://en.wikipedia.org/wiki/Stochastic_gradient_descent#Adam) for 1D Gradient Descent.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gradient Descent and Backprop for DNN\n",
    "\n",
    "Before the implementation, we must first understand how a neural network works. In a nutshell, a neural network trys to minimise the prediction error or the loss, $\\epsilon$, by tuning the model parameters $\\mathbf{w}$. **Gradient Descent** is the most fundamental algorithm for this purpose, which iteratively updates $\\mathbf{w}$ in the \"gradient\" direction $\\nabla\\epsilon=\\dfrac{\\partial \\epsilon}{\\partial \\mathbf{w}}$. This can be achieved through cycles of forward and backward propagations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Forward propagation\n",
    "\n",
    "For a fully-connected dense layer, the trainable model parameters include the **weights** and the **biases**. Let $N_l$ denote the number of neurons in the $l$-th layer. The $l$-th layer then contains $N_{l}\\times N_{l-1}$ weights and $N_l$ biases, as denoted respectively by $w^l_{ij}$ and $b^l_i$, where $1\\le i \\le N_{l}$ and $1\\le j \\le N_{l-1}$, as shown in the following figure.\n",
    "\n",
    "<img src=\"https://github.com/stfc-sciml/sciml-workshop/blob/master/course_3.0_with_solutions/markdown_pic/fwdd.png?raw=1\" width=\"40%\">\n",
    "\n",
    "The forward propagation passes the input data to the first hidden layer and computes the values on its neurons (using the current weights and biases), which are then \"**activated**\" and passed to the second layer and so on until the output layer is reached. The pre-activation value at the $i$-th neuron in the $l$-th layer, as denoted by $z^l_{i}$, is computed by\n",
    "\n",
    "$$z^l_{i}=\\sum_{j=1}^{N_{l-1}} w^l_{ij} a^{l-1}_{j} + b^l_i,\\quad i=1,2,\\cdots,N_{l},\n",
    "\\label{eq:zi} \\tag{1}\n",
    "$$\n",
    "\n",
    "where $a^{l-1}_{j}$ is the post-activation value at the $j$-th neuron in the $(l-1)$-th layer. Then the post-activation values of the $l$-th layer are computed using the given activation function $f_l$, such as `ReLU` and `sigmoid`:\n",
    "\n",
    "$$a^l_{i}=f_l\\left(z^l_{i}\\right),\\quad i=1,2,\\cdots,N_{l}.\n",
    "\\label{eq:ai} \\tag{2}\n",
    "$$\n",
    "\n",
    "Next, $a^{l}_{i}$ will be passed to the $(l+1)$-th layer to compute $z^{l+1}_{k}$, $k=1,2,\\cdots,N_{l+1}$.\n",
    "\n",
    "#### Implementation in Python\n",
    "\n",
    "Assume that we have a class called `Layer` that has properties `w` and `b` and a member function `activation_func`. The forward propagation through this layer can be implemented as follows, passing `a` from the previous layer:\n",
    "\n",
    "```python\n",
    "def forward(self, a_prev):\n",
    "    # pre-activation\n",
    "    self.z = np.dot(self.w, a_prev) + self.b\n",
    "    # post-activation\n",
    "    self.a = self.activation_func(self.z)\n",
    "    # return a to feed the next layer\n",
    "    return self.a\n",
    "```\n",
    "\n",
    "Note that we store `z` and `a` (by using `self.`) because they will be needed for backpropagation.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Backward propagation of errors\n",
    "\n",
    "Suppose that our DNN has $M$ layers in total, so the $M$-th layer will be the output layer and $a^{M}_{k}$ the final prediction. The vector-valued error is computed by subtracting the prediction $a^{M}_{k}$ and the ground truth $y_{k}$. Let us use the Mean Squared Error (MSE) as the scalar-valued loss function, i.e.,\n",
    "\n",
    "$$\\epsilon=\\dfrac{1}{N_M}\\sum_{k=1}^{N_M}(a^{M}_{k}-y_{k})^2.\n",
    "\\label{eq:eps}\\tag{3}\n",
    "$$\n",
    "\n",
    "\n",
    "The purpose of backpropagation is to find the model gradients $\\dfrac{\\partial \\epsilon}{\\partial w^l_{ij}}$ and $\\dfrac{\\partial \\epsilon}{\\partial b^l_{i}}$. They can be evaluated based on the **chain rule**:\n",
    "\n",
    "\n",
    "$$\\dfrac{\\partial \\epsilon}{\\partial w^l_{ij}}\n",
    "=\\dfrac{\\partial \\epsilon}{\\partial z^l_{i}}\\dfrac{\\partial z^l_{i}}{\\partial w^l_{ij}}\n",
    "=\\dfrac{\\partial \\epsilon}{\\partial a^l_{i}}\\dfrac{d a^l_{i}}{d z^l_{i}}\\dfrac{\\partial z^l_{i}}{\\partial w^l_{ij}}.\n",
    "\\label{eq:w}\\tag{4}\n",
    "$$\n",
    "\n",
    "The second term on the R.H.S. of eq. $\\eqref{eq:w}$, $\\dfrac{d a^l_{i}}{d z^l_{i}}$, is the derivative of the activation funciton, $f_l'\\left({z^l_{i}}\\right)$, and the third term $\\dfrac{\\partial z^l_{i}}{\\partial w^l_{ij}}$ simply $a^{l-1}_{j}$. The first term is more complicated because $a^l_{i}$ contributes to $\\epsilon$ via the whole $(l+1)$-th layer, that is,\n",
    "$\\dfrac{\\partial \\epsilon}{\\partial a^l_{i}}= \\sum\\limits_{k=1}^{N_{l+1}} \n",
    "\\dfrac{\\partial \\epsilon}{\\partial z^{l+1}_{k}}\\dfrac{\\partial z^{l+1}_{k}}{\\partial a^l_{i}}  =\n",
    "\\sum\\limits_{k=1}^{N_{l+1}} \n",
    "\\dfrac{\\partial \\epsilon}{\\partial z^{l+1}_{k}}w_{ki}^{l+1}.\n",
    "$\n",
    "Inserted with all these three terms, the above gradient can be eventually rearrange as (also considering the output layer, $l=M$):\n",
    "\n",
    "$$\\dfrac{\\partial \\epsilon}{\\partial w^l_{ij}}\n",
    "=\\dfrac{\\partial \\epsilon}{\\partial z^l_{i}}a^{l-1}_{j}\n",
    "=\\dfrac{\\partial \\epsilon}{\\partial a^l_{i}}\n",
    "f'_l\\left({z^l_{i}}\\right)a^{l-1}_{j}, \\quad \\dfrac{\\partial \\epsilon}{\\partial a^l_{i}}=\n",
    "\\begin{cases}\n",
    "\\dfrac{2}{N_M}(a^{l}_{i}-y_{i}),&l=M;\\\\\n",
    "\\sum\\limits_{k=1}^{N_{l+1}} w_{ki}^{l+1}\\dfrac{\\partial \\epsilon}{\\partial z^{l+1}_{k}},&l<M.\n",
    "\\end{cases}\n",
    "\\label{eq:wij}\\tag{5}\n",
    "$$\n",
    "\n",
    "\n",
    "Obviously, the above gradient must be computed in a backward sequence (from $l=M$ to $l=1$), with the \"errors\" $\\dfrac{\\partial \\epsilon}{\\partial z^{l+1}_{k}}$ passed from the $(l+1)$-th layer to the $l$-th layer; this is why the process is called backpropagtion. For the biases, it is straightforward to show that \n",
    "$\\dfrac{\\partial \\epsilon}{\\partial b^l_{i}}=\\dfrac{\\partial \\epsilon}{\\partial z^l_{i}}$ because \n",
    "$\\dfrac{\\partial z^l_{i}}{\\partial b^l_{i}}=1$.\n",
    "\n",
    "\n",
    "#### Implementation in Python\n",
    "\n",
    "Based on eq. $\\eqref{eq:wij}$, the backpropagation can be coded as follows:\n",
    "\n",
    "```python\n",
    "# input: 1) ∂ε/∂a of this layer but computed in the next layer\n",
    "#        2) a of the previous layer\n",
    "def backward(self, de_da, a_prev):\n",
    "    # ∂ε/∂z (which is also ∂ε/∂b)\n",
    "    de_dz = de_da * self.activation_func(self.z, derivative=True)\n",
    "    # accumulate ∂ε/∂w, ∂ε/∂b\n",
    "    self.de_dw += np.outer(de_dz, a_prev)\n",
    "    self.de_db += de_dz\n",
    "    # ∂ε/∂a to be passed to the previous layer\n",
    "    de_da_prev = np.dot(self.w.T, de_dz)\n",
    "    return de_da_prev\n",
    "```\n",
    "\n",
    "Here we accumulate the gradients (instead of directly updating the parameters) because we will employ Mini-batch Gradient Descent for parameter update, as introduced below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Mini-batch Gradient Descent\n",
    "\n",
    "Through one iteration of forward and backward propagations, each data in the dataset will yield a gradient. Then the question is which gradient we should use to update the model parameters. To answer this question, we will use an algorithm called **Mini-batch Gradient Descent**:\n",
    "\n",
    "$$\n",
    "\\Delta w_{ij}=-\\eta \\dfrac{1}{B} \\sum_{m=1}^{B}  \\dfrac{\\partial\\epsilon_m}{\\partial w_{ij}},\n",
    "\\label{eq:mini}\\tag{6}\n",
    "$$\n",
    "\n",
    "where $B$ is a given **batch size**, $\\dfrac{\\partial\\epsilon_m}{\\partial w_{ij}}$ the gradient computed with the $m$-th data in the mini-batch and $\\eta$ the **learning rate**. \n",
    "\n",
    "\n",
    "#### Batch size $B$\n",
    "\n",
    "When $B$ is selected to be the total number of data in the dataset, the algorithm is usually referred to as **Batch Gradient Descent**. For a non-convex optimisation problem (which is generally the case in deep learning), Batch Gradient Descent can easily be trapped by local minima.\n",
    "\n",
    "To help the algorithm to escape from local minima, we can add a bit noise to the trajectory of gradient descent by using a gradient averaged over a *random subset* of the dataset -- the so called **Mini-batch Stochastic Gradient Descent** or **Mini-batch Gradient Descent**. The noise level decreases with $B$. When $B=1$, the algorithm is commonly known as **Stochastic Gradient Descent**, which introduces the highest noise level and thus may suffer from slow convergence.  \n",
    "\n",
    "\n",
    "#### Learning rate $\\eta$\n",
    "Now we have found the direction to update $w^l_{ij}$ and $b^l_{i}$, but we still need to determine the magnitude of the update. This will introduce another network parameter called the **learning rate**. In our implementation, we will use a constant learning rate. In applications, it is usually more efficient to use an adaptive learning rate, such as by using the Adam optimiser. \n",
    "\n",
    "> \"The learning rate is perhaps the most important hyperparameter. If you have time to tune only one hyperparameter, tune the learning rate.\" -- Goodfellow, Deep Learning\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implementation from Scratch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Activation functions\n",
    "\n",
    "We first define some activation functions. They can be passed as an argument to create a `Layer` object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# relu\n",
    "def relu(x, derivative=False):\n",
    "    if derivative:\n",
    "        return 1. * (x > 0)\n",
    "    else:\n",
    "        return x * (x > 0.)\n",
    "\n",
    "# sigmoid\n",
    "def sigmoid(x, derivative=False):\n",
    "    if derivative:\n",
    "        s = sigmoid(x)\n",
    "        return s * (1. - s)\n",
    "    else:\n",
    "        return 1. / (1. + np.exp(-x))\n",
    "    \n",
    "# linear\n",
    "def linear(x, derivative=False):\n",
    "    if derivative:\n",
    "        return np.full(x.shape, 1.)\n",
    "    else:\n",
    "        return x.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Class `Layer`\n",
    "The `Layer` class is where the model parameters and neuron values are stored and the layer-wise operations happen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# layer class\n",
    "class Layer:\n",
    "    # constructor\n",
    "    def __init__(self, size, activation=linear, name='Untitled_Layer'):\n",
    "        # we cannot allocate data here because \n",
    "        # the size of the previous layer is unknown\n",
    "        self.size = size\n",
    "        self.activation_func = activation\n",
    "        self.name = name\n",
    "        \n",
    "    # intialise parameters\n",
    "    def initParameters(self, prev_size):\n",
    "        # weights and biases,\n",
    "        # intialised with random numbers from the “standard normal” distribution\n",
    "        self.w = np.random.randn(self.size, prev_size) * np.sqrt(1. / self.size)\n",
    "        self.b = np.zeros((self.size,))\n",
    "        \n",
    "        # neuron values\n",
    "        self.a = np.zeros((self.size,))\n",
    "        self.z = np.zeros((self.size,))\n",
    "        \n",
    "        # accumulated gradients\n",
    "        self.de_dw = np.zeros((self.size, prev_size))\n",
    "        self.de_db = np.zeros((self.size,))\n",
    "        \n",
    "    # forward propagation\n",
    "    def forward(self, a_prev):\n",
    "        # pre-activation\n",
    "        self.z = np.dot(self.w, a_prev) + self.b\n",
    "        # post-activation\n",
    "        self.a = self.activation_func(self.z)\n",
    "        # return a to feed the next layer\n",
    "        return self.a\n",
    "    \n",
    "    # backward propagation\n",
    "    def backward(self, de_da, a_prev):\n",
    "        # ∂ε/∂z (which is also ∂ε/∂b)\n",
    "        de_dz = de_da * self.activation_func(self.z, derivative=True)\n",
    "        # accumulate ∂ε/∂w, ∂ε/∂b\n",
    "        self.de_dw += np.outer(de_dz, a_prev)\n",
    "        self.de_db += de_dz\n",
    "        # ∂ε/∂a to be passed to the previous layer\n",
    "        de_da_prev = np.dot(self.w.T, de_dz)\n",
    "        return de_da_prev\n",
    "    \n",
    "    # update parameters\n",
    "    def updateParameters(self, learning_rate, batch_size):\n",
    "        # update\n",
    "        self.w -= (learning_rate / batch_size) * self.de_dw\n",
    "        self.b -= (learning_rate / batch_size) * self.de_db\n",
    "        \n",
    "        # reset accumulated for the next mini-batch\n",
    "        self.de_dw.fill(0.)\n",
    "        self.de_db.fill(0.)\n",
    "        \n",
    "    # print\n",
    "    def __str__(self):\n",
    "        s = '%s\\n' % self.name\n",
    "        s += 'Size = %d\\n' % self.size\n",
    "        s += 'Activation = %s\\n' % (self.activation_func.__name__,)\n",
    "        try:\n",
    "            s += 'Number of weights = %d\\n' % self.w.size\n",
    "            s += 'Number of biases = %d\\n' % self.b.size\n",
    "        except:\n",
    "            s += 'Not set in a model.\\n'\n",
    "        return s"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Class `Model`\n",
    "\n",
    "The `Model` class contains many `Layer` objects and controls the workflow of training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model class\n",
    "class Model:\n",
    "    # constructor\n",
    "    def __init__(self, input_size, layers, name='Untitled_Model'):\n",
    "        self.input_size = input_size\n",
    "        self.layers = layers\n",
    "        self.name = name\n",
    "        \n",
    "        # initialise layers\n",
    "        prev_size = input_size\n",
    "        for layer in self.layers:\n",
    "            layer.initParameters(prev_size)\n",
    "            prev_size = layer.size\n",
    "            \n",
    "    # forward propagation\n",
    "    def forward(self, input_data):\n",
    "        a_prev = input_data.copy()\n",
    "        for layer in self.layers:\n",
    "            a_prev = layer.forward(a_prev)\n",
    "        return a_prev\n",
    "    \n",
    "    # backward propagation\n",
    "    def backward(self, input_data, error):\n",
    "        de_da = error.copy()\n",
    "        # from the last to the second layers\n",
    "        for ilayer in np.arange(len(self.layers) - 1, 0, -1):\n",
    "            de_da = self.layers[ilayer].backward(de_da, self.layers[ilayer - 1].a)\n",
    "        # the first layer\n",
    "        self.layers[0].backward(de_da, input_data)\n",
    "        \n",
    "    # update parameters\n",
    "    def updateParameters(self, learning_rate, batch_size):\n",
    "        for layer in self.layers:\n",
    "            layer.updateParameters(learning_rate, batch_size)\n",
    "    \n",
    "    # compute loss and accuracy for history\n",
    "    def computeLossAccHistory(self, xs, ys, history, key_prefix, iep):\n",
    "        for x, y in zip(xs, ys):\n",
    "            pred_y = self.forward(x)\n",
    "            history[key_prefix + 'loss'][iep] += np.linalg.norm(pred_y - y) / len(pred_y)\n",
    "            history[key_prefix + 'acc'][iep] += int(np.argmax(pred_y) == np.argmax(y))\n",
    "        history[key_prefix + 'loss'][iep] /= xs.shape[0]\n",
    "        history[key_prefix + 'acc'][iep] /= xs.shape[0]\n",
    "            \n",
    "    # train\n",
    "    def train(self, train_x, train_y, epochs=2, batch_size=32, learning_rate=0.1, validation_data=None, verbose=1):\n",
    "        # number of data\n",
    "        ndata = train_x.shape[0]\n",
    "        \n",
    "        # number of data in mini-batches\n",
    "        n_mini_batch = ndata // batch_size + int(ndata % batch_size > 0)\n",
    "        n_data_mini_batches = np.full((n_mini_batch,), batch_size)\n",
    "        # the last one may have fewer\n",
    "        n_data_mini_batches[-1] = ndata - (n_mini_batch - 1) * batch_size\n",
    "        \n",
    "        # history\n",
    "        history = {'loss': np.zeros((epochs,)), 'acc': np.zeros((epochs,))}\n",
    "        if validation_data is not None:\n",
    "            history['val_loss'] = np.zeros((epochs,))\n",
    "            history['val_acc'] = np.zeros((epochs,))\n",
    "        \n",
    "        # epoch loop\n",
    "        start_time = time.time()\n",
    "        for iep in np.arange(epochs):\n",
    "            # data must be shuffled before each epoch\n",
    "            permute = np.random.permutation(ndata)\n",
    "            train_x_sh = train_x[permute].copy()\n",
    "            train_y_sh = train_y[permute].copy()\n",
    "            \n",
    "            # mini-batch loop\n",
    "            for ibatch in np.arange(n_mini_batch):\n",
    "                # data loop\n",
    "                for idata in np.arange(ibatch * batch_size, n_data_mini_batches[ibatch]):\n",
    "                    # forward\n",
    "                    pred_y = self.forward(train_x_sh[idata])\n",
    "                    # compute MSE\n",
    "                    error = (pred_y - train_y_sh[idata]) * 2. / len(pred_y)\n",
    "                    # backward\n",
    "                    self.backward(train_x_sh[idata], error)\n",
    "                # update parameters\n",
    "                self.updateParameters(learning_rate, batch_size)\n",
    "            \n",
    "            # history on training data\n",
    "            self.computeLossAccHistory(train_x, train_y, history, '', iep)\n",
    "            \n",
    "            # history on validation data\n",
    "            if validation_data is not None:\n",
    "                self.computeLossAccHistory(validation_data[0], validation_data[1], history, 'val_', iep)\n",
    "                \n",
    "            # print training info\n",
    "            if verbose > 0 and (iep % verbose == 0 or iep == epochs - 1):\n",
    "                print('Epoch %d: ' % iep, end='')\n",
    "                print('loss = %f; acc = %f' % (history['loss'][iep], history['acc'][iep]), end='')\n",
    "                if validation_data is not None:\n",
    "                    print('; val_loss = %f; val_acc = %f; ' % (history['val_loss'][iep], \n",
    "                                                             history['val_acc'][iep]), end='')\n",
    "                print('elapsed time = %f' % (time.time() - start_time,))\n",
    "        if (verbose > 0):\n",
    "            print('Finished %d epochs, elapsed time = %f' % (epochs, time.time() - start_time))\n",
    "            \n",
    "        # return history\n",
    "        return history\n",
    "    \n",
    "    # predict\n",
    "    def predict(self, pred_x):\n",
    "        pred_y = []\n",
    "        for x in zip(pred_x):\n",
    "            pred_y.append(self.forward(x[0]))\n",
    "        return np.array(pred_y) \n",
    "    \n",
    "    # print\n",
    "    def __str__(self):\n",
    "        s = 'Model name: %s\\n' % self.name\n",
    "        s += 'Input size = %d\\n' % (self.input_size,)\n",
    "        s += 'Number of layers = %d\\n' % (len(self.layers),)\n",
    "        s += '========================================\\n'\n",
    "        for ilayer, layer in enumerate(self.layers):\n",
    "            s += 'Layer %d: ' % ilayer\n",
    "            s += str(layer)\n",
    "            s += '----------------------------------------\\n'\n",
    "        return s"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Application\n",
    "\n",
    "Now we can use our own DNN to classify the `FashionMNIST` dataset as we did in [DNN_basics_pytorch.ipynb](DNN_basics_pytorch.ipynb) based on PyTorch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision.datasets import FashionMNIST\n",
    "from torchvision.transforms import ToTensor\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 0. Load  the dataset\n",
    "\n",
    "Our implementation does not do input flattening and output one-hot encoding internally, so we have to do them manually after loading the data. Also, because our implementation is unoptimized, we will only use 20% of the dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load dataset\n",
    "train_dataset = FashionMNIST(\n",
    "    root=\"data\",\n",
    "    train=True,\n",
    "    download=True,\n",
    ")\n",
    "\n",
    "test_dataset = FashionMNIST(\n",
    "    root=\"data\",\n",
    "    train=False,\n",
    "    download=True,\n",
    ")\n",
    "\n",
    "train_images = train_dataset.data.numpy()\n",
    "train_labels = train_dataset.targets.numpy()\n",
    "\n",
    "test_images = test_dataset.data.numpy()\n",
    "test_labels = test_dataset.targets.numpy()\n",
    "\n",
    "# normalise images\n",
    "train_images = train_images / 255.0\n",
    "test_images = test_images / 255.0\n",
    "\n",
    "# flatten images by numpy.ndarray.reshape()\n",
    "npix = train_images.shape[1]\n",
    "input_size = npix * npix\n",
    "train_images = train_images.reshape((train_images.shape[0], input_size))\n",
    "test_images = test_images.reshape((test_images.shape[0], input_size))\n",
    "\n",
    "# one-hot encoding for labels by numpy.eye()\n",
    "train_labels = np.eye(10)[train_labels]\n",
    "test_labels = np.eye(10)[test_labels]\n",
    "\n",
    "# use 20% of the dataset\n",
    "train_images = train_images[0:12000]\n",
    "train_labels = train_labels[0:12000]\n",
    "test_images = test_images[0:2000]\n",
    "test_labels = test_labels[0:2000]\n",
    "\n",
    "# print info\n",
    "print(\"Number of training samples: %d\" % len(train_images))\n",
    "print(\"Number of test samples: %d\" % len(test_images))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Build the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# layers\n",
    "hidden_layer = Layer(128, activation=relu, name='Hidden')\n",
    "output_layer = Layer(10, activation=sigmoid, name='Output')\n",
    "\n",
    "# model\n",
    "model = Model(input_size, [hidden_layer, output_layer], name=\"DNN for fashion-mnist\")\n",
    "\n",
    "# print summary\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Train the model\n",
    "\n",
    "Here we need a large number of epochs (1000) to obtain an accuracy comparable to what we had done in [DNN_basics.ipynb](DNN_basics.ipynb) with 50 epochs. Given that we only use 20% of the dataset, our Mini-batch Gradient Descent algorithm is still three times less efficient in convergence than the Adam optimiser implemented in Keras. Use a smaller `epochs` (100~200) for a faster but less accurate result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train the model\n",
    "history = model.train(train_images, train_labels, \n",
    "                      epochs=200, batch_size=32, learning_rate=0.3,\n",
    "                      validation_data=(test_images, test_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot accuracy\n",
    "plt.figure(dpi=100, figsize=(12, 4))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(history['acc'], label='Accuracy on training data')\n",
    "plt.plot(history['val_acc'], label='Accuracy on test data')\n",
    "plt.legend()\n",
    "plt.title(\"Accuracy\")\n",
    "\n",
    "# plot loss\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(history['loss'], label='Loss on training data')\n",
    "plt.plot(history['val_loss'], label='Loss on test data')\n",
    "plt.legend()\n",
    "plt.title(\"Loss\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Make predictions\n",
    "\n",
    "After training, we can use `model.predict` to make predictions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make predictions\n",
    "pred_labels = model.predict(test_images)\n",
    "print('Number of test images: %d' % test_images.shape[0])\n",
    "print('Number of correct predictions: %d' % \n",
    "      np.where(np.argmax(pred_labels, axis=1) == np.argmax(test_labels, axis=1))[0].size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercises\n",
    "\n",
    "* **Easy**: Add an option to `Model` class to allow for a different method to initialise weights and biases, such as Glorot uniform [original paper](http://proceedings.mlr.press/v9/glorot10a/glorot10a.pdf). \n",
    "\n",
    "* **Medium**: Add the dropout rate as a property of `Layer`. Randomly select neurons based on this rate and drop them out by zeroing their values in both forward and backward propagations. \n",
    "\n",
    "* **Hard**: Implement Implement [AdaGrad](https://en.wikipedia.org/wiki/Stochastic_gradient_descent#AdaGrad), [RMSProp](https://en.wikipedia.org/wiki/Stochastic_gradient_descent#RMSProp) or [Adam](https://en.wikipedia.org/wiki/Stochastic_gradient_descent#Adam)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:sciml-workshop] *",
   "language": "python",
   "name": "conda-env-sciml-workshop-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
