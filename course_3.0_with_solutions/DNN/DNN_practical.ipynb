{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DNN Practical: Ag Detection by Muon Spectroscopy\n",
    "\n",
    "In this notebook, we attempt to solve a real problem in physics using a fully connected DNN.\n",
    "\n",
    "We have a set of spectra from Muon spectroscopy experiments, from which we would like to detect whether or not a certain element is present in a sample. In this notebook, we are going to train a neural network to detect the presence of Ag. Through this practice, we will encounter and overcome a pitfall in deep learning known as **class imbalance**. We will also explore **early stopping** and saving checkpoints from the best performing model.\n",
    "\n",
    "## About the data\n",
    "\n",
    "The data in this example is generated from simulated muon spectroscopy experiments. First the data was generated for each individual element by simulating the spectral emmission lines of that element. Then for the mixed coumpounds the different elemental spectra were mixed in proportion to how much of that element is present in the compound. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Google Cloud Storage Boilerplate\n",
    "\n",
    "The following two cells have some boilerplate to mount the Google Cloud Storage bucket containing the data used for this notebook to your Google Colab file system. **Even you are not using Google Colab, please make sure you run these two cells.** \n",
    "\n",
    "To access the data from Google Colab, you need to:\n",
    "\n",
    "1. Run the first cell;\n",
    "2. Follow the link when prompted (you may be asked to log in with your Google account);\n",
    "3. Copy the Google SDK token back into the prompt and press `Enter`;\n",
    "4. Run the second cell and wait until the data folder appears.\n",
    "\n",
    "If everything works correctly, a new folder called `sciml-workshop-data` should appear in the file browser on the left. Depending on the network speed, this may take one or two minutes. Ignore the warning \"You do not appear to have access to project ...\". If you are running the notebook locally or you have already connected to the bucket, these cells will have no side effects."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7f11cd811eb0>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os \n",
    "# import Pytorch libraries\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "# helpers\n",
    "import time\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "plt.style.use('ggplot')\n",
    "\n",
    "# need some certainty in data processing\n",
    "np.random.seed(0)\n",
    "torch.manual_seed(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using Colab: false, Data path: /mnt/materials/SciML/sciml-workshop/, exist: False\n"
     ]
    }
   ],
   "source": [
    "# variables passed to bash; do not change\n",
    "project_id = 'sciml-workshop'\n",
    "bucket_name = 'sciml-workshop-data'\n",
    "colab_data_path = '/content/sciml-workshop-data/'\n",
    "\n",
    "try:\n",
    "    from google.colab import auth\n",
    "    auth.authenticate_user()\n",
    "    google_colab_env = 'true'\n",
    "    data_path = colab_data_path\n",
    "except:\n",
    "    google_colab_env = 'false'\n",
    "    ###################################################\n",
    "    ######## specify your local data path here ########\n",
    "    ###################################################\n",
    "    with open('../local_data_path.txt', 'r') as f: \n",
    "        data_path = f.read().splitlines()[0]\n",
    "\n",
    "\n",
    "print(f\"Using Colab: {google_colab_env}, Data path: {data_path}, exist: {os.path.isdir(data_path)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mount the workshop data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running notebook locally.\n",
      "Mounting bucket sciml-workshop-data to /mnt/materials/SciML/sciml-workshop/\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "s3fs: unable to access MOUNTPOINT /mnt/materials/SciML/sciml-workshop/: No such file or directory\n"
     ]
    },
    {
     "ename": "CalledProcessError",
     "evalue": "Command 'b'\\n# running locally\\nif ! $1; then\\n    echo \"Running notebook locally.\"\\n    #exit\\nfi\\n\\nif ! command -v s3fs &> /dev/null\\nthen\\n    echo \"Unable to find s3fs. Installing ...\"\\n    apt -qq update\\n    apt -qq install s3fs fuse\\nfi\\n\\necho \"Mounting bucket $3 to $2\"\\ns3fs $3 $2 -o allow_other,nonempty,use_path_request_style,no_check_certificate,public_bucket=1,ssl_verify_hostname=0,host=https://s3.echo.stfc.ac.uk,url=https://s3.echo.stfc.ac.uk\\n'' returned non-zero exit status 1.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mCalledProcessError\u001b[0m                        Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[18], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mget_ipython\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_cell_magic\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mbash\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m-s \u001b[39;49m\u001b[38;5;132;43;01m{google_colab_env}\u001b[39;49;00m\u001b[38;5;124;43m \u001b[39;49m\u001b[38;5;132;43;01m{data_path}\u001b[39;49;00m\u001b[38;5;124;43m \u001b[39;49m\u001b[38;5;132;43;01m{bucket_name}\u001b[39;49;00m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m# running locally\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43mif ! $1; then\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m    echo \u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mRunning notebook locally.\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m    #exit\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43mfi\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43mif ! command -v s3fs &> /dev/null\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43mthen\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m    echo \u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mUnable to find s3fs. Installing ...\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m    apt -qq update\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m    apt -qq install s3fs fuse\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43mfi\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43mecho \u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mMounting bucket $3 to $2\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43ms3fs $3 $2 -o allow_other,nonempty,use_path_request_style,no_check_certificate,public_bucket=1,ssl_verify_hostname=0,host=https://s3.echo.stfc.ac.uk,url=https://s3.echo.stfc.ac.uk\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/sciml-workshop/lib/python3.10/site-packages/IPython/core/interactiveshell.py:2517\u001b[0m, in \u001b[0;36mInteractiveShell.run_cell_magic\u001b[0;34m(self, magic_name, line, cell)\u001b[0m\n\u001b[1;32m   2515\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbuiltin_trap:\n\u001b[1;32m   2516\u001b[0m     args \u001b[38;5;241m=\u001b[39m (magic_arg_s, cell)\n\u001b[0;32m-> 2517\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2519\u001b[0m \u001b[38;5;66;03m# The code below prevents the output from being displayed\u001b[39;00m\n\u001b[1;32m   2520\u001b[0m \u001b[38;5;66;03m# when using magics with decorator @output_can_be_silenced\u001b[39;00m\n\u001b[1;32m   2521\u001b[0m \u001b[38;5;66;03m# when the last Python token in the expression is a ';'.\u001b[39;00m\n\u001b[1;32m   2522\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(fn, magic\u001b[38;5;241m.\u001b[39mMAGIC_OUTPUT_CAN_BE_SILENCED, \u001b[38;5;28;01mFalse\u001b[39;00m):\n",
      "File \u001b[0;32m~/miniconda3/envs/sciml-workshop/lib/python3.10/site-packages/IPython/core/magics/script.py:154\u001b[0m, in \u001b[0;36mScriptMagics._make_script_magic.<locals>.named_script_magic\u001b[0;34m(line, cell)\u001b[0m\n\u001b[1;32m    152\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    153\u001b[0m     line \u001b[38;5;241m=\u001b[39m script\n\u001b[0;32m--> 154\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshebang\u001b[49m\u001b[43m(\u001b[49m\u001b[43mline\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcell\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/sciml-workshop/lib/python3.10/site-packages/IPython/core/magics/script.py:314\u001b[0m, in \u001b[0;36mScriptMagics.shebang\u001b[0;34m(self, line, cell)\u001b[0m\n\u001b[1;32m    309\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m args\u001b[38;5;241m.\u001b[39mraise_error \u001b[38;5;129;01mand\u001b[39;00m p\u001b[38;5;241m.\u001b[39mreturncode \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m    310\u001b[0m     \u001b[38;5;66;03m# If we get here and p.returncode is still None, we must have\u001b[39;00m\n\u001b[1;32m    311\u001b[0m     \u001b[38;5;66;03m# killed it but not yet seen its return code. We don't wait for it,\u001b[39;00m\n\u001b[1;32m    312\u001b[0m     \u001b[38;5;66;03m# in case it's stuck in uninterruptible sleep. -9 = SIGKILL\u001b[39;00m\n\u001b[1;32m    313\u001b[0m     rc \u001b[38;5;241m=\u001b[39m p\u001b[38;5;241m.\u001b[39mreturncode \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m9\u001b[39m\n\u001b[0;32m--> 314\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m CalledProcessError(rc, cell)\n",
      "\u001b[0;31mCalledProcessError\u001b[0m: Command 'b'\\n# running locally\\nif ! $1; then\\n    echo \"Running notebook locally.\"\\n    #exit\\nfi\\n\\nif ! command -v s3fs &> /dev/null\\nthen\\n    echo \"Unable to find s3fs. Installing ...\"\\n    apt -qq update\\n    apt -qq install s3fs fuse\\nfi\\n\\necho \"Mounting bucket $3 to $2\"\\ns3fs $3 $2 -o allow_other,nonempty,use_path_request_style,no_check_certificate,public_bucket=1,ssl_verify_hostname=0,host=https://s3.echo.stfc.ac.uk,url=https://s3.echo.stfc.ac.uk\\n'' returned non-zero exit status 1."
     ]
    }
   ],
   "source": [
    "%%bash -s {google_colab_env} {data_path} {bucket_name}\n",
    "\n",
    "# running locally\n",
    "if ! $1; then\n",
    "    echo \"Running notebook locally.\"\n",
    "    #exit\n",
    "fi\n",
    "\n",
    "if ! command -v s3fs &> /dev/null\n",
    "then\n",
    "    echo \"Unable to find s3fs. Installing ...\"\n",
    "    apt -qq update\n",
    "    apt -qq install s3fs fuse\n",
    "fi\n",
    "\n",
    "echo \"Mounting bucket $3 to $2\"\n",
    "s3fs $3 $2 -o allow_other,nonempty,use_path_request_style,no_check_certificate,public_bucket=1,ssl_verify_hostname=0,host=https://s3.echo.stfc.ac.uk,url=https://s3.echo.stfc.ac.uk"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### If there is error mounting the bucket, manually verify that the s3fs command is correct by running the command in a new terminal;\n",
    "```bash\n",
    "sudo mkdir /mnt/materials/SciML/sciml-workshop/\n",
    "sudo chmod \n",
    "sudo s3fs sciml-workshop-data /mnt/materials/SciML/sciml-workshop/ -o allow_other,nonempty,use_path_request_style,\\\n",
    "no_check_certificate,public_bucket=1,ssl_verify_hostname=0,host=https://s3.echo.stfc.ac.uk,url=https://s3.echo.stfc.ac.uk\n",
    "```\n",
    "### Use umount option with -l to unmount the bucket\n",
    "```bash\n",
    "sudo umount -l /mnt/materials/SciML/sciml-workshop/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The dataset\n",
    "\n",
    "### Read raw data\n",
    "\n",
    "The raw data, which include the constituent elements and the Muon spectra of the samples, are stored in the pickle file `muon/Ag_muon_data.pkl`. We load this file into a `pandas` dataframe and take a quick look."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of samples in the dataset: 138613\n",
      "Length of spectra for each sample: 1000\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Elements</th>\n",
       "      <th>oh</th>\n",
       "      <th>c</th>\n",
       "      <th>Spectra</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>8877</th>\n",
       "      <td>[Si, Fe, Sb, Bi]</td>\n",
       "      <td>0</td>\n",
       "      <td>[0, 6, 12, 18, 24, 30, 36, 42, 48, 54, 60, 66,...</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 132.19327401887...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>88555</th>\n",
       "      <td>[Sb, Fe, Bi]</td>\n",
       "      <td>0</td>\n",
       "      <td>[0, 6, 12, 18, 24, 30, 36, 42, 48, 54, 60, 66,...</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 106.91719527298...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>111153</th>\n",
       "      <td>[Si, Sb]</td>\n",
       "      <td>0</td>\n",
       "      <td>[0, 6, 12, 18, 24, 30, 36, 42, 48, 54, 60, 66,...</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 108.04741040971...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27279</th>\n",
       "      <td>[Fe, Cu, Bi]</td>\n",
       "      <td>0</td>\n",
       "      <td>[0, 6, 12, 18, 24, 30, 36, 42, 48, 54, 60, 66,...</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 106.84704457348...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26419</th>\n",
       "      <td>[Si, Fe]</td>\n",
       "      <td>0</td>\n",
       "      <td>[0, 6, 12, 18, 24, 30, 36, 42, 48, 54, 60, 66,...</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 100.82180543319...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                Elements  oh  \\\n",
       "8877    [Si, Fe, Sb, Bi]   0   \n",
       "88555       [Sb, Fe, Bi]   0   \n",
       "111153          [Si, Sb]   0   \n",
       "27279       [Fe, Cu, Bi]   0   \n",
       "26419           [Si, Fe]   0   \n",
       "\n",
       "                                                        c  \\\n",
       "8877    [0, 6, 12, 18, 24, 30, 36, 42, 48, 54, 60, 66,...   \n",
       "88555   [0, 6, 12, 18, 24, 30, 36, 42, 48, 54, 60, 66,...   \n",
       "111153  [0, 6, 12, 18, 24, 30, 36, 42, 48, 54, 60, 66,...   \n",
       "27279   [0, 6, 12, 18, 24, 30, 36, 42, 48, 54, 60, 66,...   \n",
       "26419   [0, 6, 12, 18, 24, 30, 36, 42, 48, 54, 60, 66,...   \n",
       "\n",
       "                                                  Spectra  \n",
       "8877    [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 132.19327401887...  \n",
       "88555   [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 106.91719527298...  \n",
       "111153  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 108.04741040971...  \n",
       "27279   [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 106.84704457348...  \n",
       "26419   [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 100.82180543319...  "
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# read data\n",
    "df = pd.read_pickle(data_path + 'sciml-workshop/muon-data/ag-muon-data-tight.pkl')\n",
    "#print dimensions\n",
    "print('Number of samples in the dataset: %d' % len(df['Spectra']))\n",
    "print('Length of spectra for each sample: %d' % len(df['Spectra'][0]))\n",
    "\n",
    "# print the first few data\n",
    "df.head(n=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the above table, the `Elements` and the `Spectra` columns show respectively the elements and the spectra of the samples. There are 138,613 samples in the dataset, and each spectrum is a series of 1000 positive reals. \n",
    "\n",
    "To get a feel for the complexity of picking out signals with Ag in multinary samples, we can plot some random spectra for three representative cases: \n",
    "\n",
    "* no Ag\n",
    "* pure Ag\n",
    "* Ag-Si binary\n",
    "\n",
    "Note that we are plotting only the first part of each spectrum. Change `[0:150]` to `[:]` to show the full spectra."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# conditions to select data\n",
    "conditions = [\n",
    "# no Ag\n",
    "('no Ag', np.where(['Ag' not in elements for elements in df['Elements']])[0]),\n",
    "# pure Ag\n",
    "('pure Ag', np.where([['Ag'] == elements for elements in df['Elements']])[0]),\n",
    "# Ag-Si\n",
    "('Ag-Si binary', np.where([['Ag', 'Si'] == elements for elements in df['Elements']])[0])\n",
    "]\n",
    "\n",
    "# plot\n",
    "ncond = len(conditions)\n",
    "nplot = 4 # number of plots per condition\n",
    "fig, axs = plt.subplots(nplot, ncond, dpi=200, figsize=(ncond * 5, nplot * 2), sharex=True, sharey=True)\n",
    "plt.subplots_adjust(wspace=.1, hspace=.2)\n",
    "for icond, cond in enumerate(conditions):\n",
    "    for iplot, idata in enumerate(np.random.choice(cond[1], nplot)):\n",
    "        axs[iplot, icond].plot(df['Spectra'][idata][150:700], c='C%d' % icond)\n",
    "        axs[iplot, icond].set_xlabel('Sample %d: %s' % (idata, cond[0]), c='C%d' % icond)\n",
    "        axs[iplot, icond].set_ylim(0, 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extract training data\n",
    "\n",
    "The input data for our network will be the `Spectra` column, and we can use the `to_list()` method to convert it to a numpy array. The output data for our network will be a binary-valued one-hot vector: **0 for no Ag** in the sample and **1 otherwise**. One-hot encoding can be achieved by a simple for-loop. Also, it is important to normalise each spectrum between 0 and 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###### input ######\n",
    "# convert the 'Spectra' column to numpy\n",
    "train_x = np.array(df['Spectra'].to_list())\n",
    "# normalise each spectrum to [0, 1]\n",
    "train_x /= np.max(train_x, axis=1)[:, np.newaxis]\n",
    "\n",
    "###### output ######\n",
    "# one-hot encoding: whether Ag is in 'Elements'\n",
    "train_y = np.array(['Ag' in elements for elements in df['Elements']]).astype(np.int32)[:, np.newaxis]\n",
    "\n",
    "# print data shapes\n",
    "print(\"Shape of input: %s\" % str(train_x.shape))\n",
    "print(\"Shape of output: %s\" % str(train_y.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ag-detection by DNN\n",
    "\n",
    "## 1. Try out a network\n",
    "\n",
    "\n",
    "### Build the network\n",
    "\n",
    "Based on what we have learnt in [DNN_basics.ipynb](DNN_basics.ipynb), design a simple neural network with `Linear` layers to detect Ag in the spectra. In general, it is not a straightforward task to determine the number of hidden layers and the number of neurons in each layer, which usually involves some trial and error. In this case, our input size is 1000 and output size is 1, so we'd better add a small layer before it, such as one with size 64.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model(nn.Module):\n",
    "    def __init__(self, input_dim=1000, output_dim=1):\n",
    "        super().__init__()\n",
    "        self.input_layer = nn.Linear(input_dim, 64)\n",
    "        self.input_relu = nn.ReLU()\n",
    "        \n",
    "        self.hidden_layer = nn.Linear(64, 16)\n",
    "        self.hidden_relu = nn.ReLU()\n",
    "        \n",
    "        self.out_layer = nn.Linear(16, output_dim)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "    \n",
    "    def forward(self, x):\n",
    "        \n",
    "        x = self.input_layer(x)\n",
    "        x = self.input_relu(x)\n",
    "        \n",
    "        x = self.hidden_layer(x)\n",
    "        x = self.hidden_relu(x)\n",
    "        \n",
    "        x = self.out_layer(x)\n",
    "        x = self.sigmoid(x)\n",
    "        \n",
    "        return x\n",
    "\n",
    "# create a instance of the model\n",
    "model = Model()\n",
    "# print model summary\n",
    "print(model)\n",
    "\n",
    "# test the forward pass is working using dummy input\n",
    "print(model(torch.randn(1, 1000)).detach().item())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create dataloaders \n",
    "We will convert the Numpy based train data to Pytorch dataloader suitable for our trainer function. We will also need to create a subset of data for validation by using the *final* 20% of the dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dataloaders(train_x, train_y, batch_size=64, val_split=0.2):\n",
    "    \n",
    "    idx = int((1-val_split)*len(train_x))\n",
    "\n",
    "    train_dataset = TensorDataset(\n",
    "        torch.tensor(train_x[:idx], dtype=torch.float32), \n",
    "        torch.tensor(train_y[:idx], dtype=torch.float32)\n",
    "        )\n",
    "    test_dataset = TensorDataset(\n",
    "        torch.tensor(train_x[idx:], dtype=torch.float32),\n",
    "        torch.tensor(train_y[idx:], dtype=torch.float32)\n",
    "        )\n",
    "\n",
    "    train_dataloader = DataLoader(train_dataset, batch_size=batch_size)\n",
    "    test_dataloader = DataLoader(test_dataset, batch_size=batch_size)\n",
    "    \n",
    "    print(f\"Train samples:{len(train_dataloader.dataset)}, Test samples:{len(test_dataloader.dataset)}, Batch size:{batch_size}\")\n",
    "    \n",
    "    return train_dataloader, test_dataloader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build optimizer and loss function\n",
    "\n",
    "We can keep using `Adam` for the `optimizer`. For the `loss`, since we are fitting to a range between 0 and 1, we can choose binary crossentropy `BCELoss`.\n",
    "\n",
    "**Suggested Answer** \n",
    "\n",
    "<details> <summary>Show / Hide</summary> \n",
    "<p>\n",
    "    \n",
    "```python\n",
    "# define loss function\n",
    "loss_fn = nn.BCELoss()\n",
    "# create a optimizer with constant learning rate of 0.001\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "```\n",
    "</p>\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create our  trainer function\n",
    "\n",
    "As in the [DNN_basics.ipynb](DNN_basics.ipynb), we will create a custom trainer function to simplify the training and testing our model. We will initiate our Pytorch model, optimizers and loss inside the trainer so that we only need to our dataset and number of epochs. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def binary_accuracy(pred, ground_truth):\n",
    "    return torch.where(torch.abs(pred - ground_truth) < .1)[0].numel() / pred.numel()\n",
    "\n",
    "def train(dataloader, model, loss_fn, optimizer, device):\n",
    "\n",
    "    model.train()\n",
    "    \n",
    "    train_loss, train_accuracy = 0, 0\n",
    "    for num_batches, (X, y) in enumerate(dataloader):\n",
    "\n",
    "        # place tensors to device\n",
    "        X, y = X.to(device), y.to(device)\n",
    "\n",
    "        # Compute prediction error\n",
    "        pred_y = model(X)\n",
    "\n",
    "        # compute loss, default mean\n",
    "        loss = loss_fn(pred_y, y)\n",
    "\n",
    "        # Backpropagation\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # compute test loss and accuracy with threshold 0.5\n",
    "        train_loss += loss.detach().item()\n",
    "        train_accuracy += binary_accuracy(pred_y, y)\n",
    "        \n",
    "    train_loss /= num_batches\n",
    "    train_accuracy /= num_batches\n",
    "    \n",
    "    return train_loss, train_accuracy\n",
    "\n",
    "def test(dataloader, model, loss_fn, device):\n",
    "    # set the model to eval mode\n",
    "    model.eval()\n",
    "    \n",
    "    test_loss, test_accuracy = 0, 0\n",
    "    with torch.no_grad():\n",
    "        for num_batches, (X, y) in enumerate(dataloader):\n",
    "            \n",
    "            # place tensors to device\n",
    "            X, y = X.to(device), y.to(device)\n",
    "            \n",
    "            # inference/prediction\n",
    "            pred_y = model(X)\n",
    "\n",
    "            # compute test loss and accuracy\n",
    "            test_loss += loss_fn(pred_y, y).item()\n",
    "            test_accuracy += binary_accuracy(pred_y, y)\n",
    "            \n",
    "    test_loss /= num_batches\n",
    "    test_accuracy /= num_batches\n",
    "\n",
    "    return test_loss, test_accuracy\n",
    "\n",
    "def trainer(model, train_x, train_y, batch_size, epochs, learning_rate, device):\n",
    "    \n",
    "    model = model.to(device)\n",
    "    \n",
    "    loss_fn = nn.BCELoss()\n",
    "    \n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "    \n",
    "    train_dataloader, test_dataloader = create_dataloaders(train_x, train_y, batch_size)\n",
    "\n",
    "    # bucket to hold history\n",
    "    h = dict(train_loss=[], test_loss=[], train_accuracy=[], test_accuracy=[])\n",
    "    \n",
    "    # start timer\n",
    "    start = time.time()\n",
    "    \n",
    "    for t in range(epochs):\n",
    "        print(f\"\\nEpoch {t+1}/{epochs}: \", end='')\n",
    "        \n",
    "        train_loss, train_accuracy = train(train_dataloader, model, loss_fn, optimizer, device)\n",
    "\n",
    "        test_loss, test_accuracy = test(test_dataloader, model, loss_fn, device)\n",
    "        \n",
    "        print(f\"train_loss: {train_loss:0.4f}, test_loss: {test_loss:0.4f}, train_accuracy: {(100*train_accuracy):0.2f}%, test_accuracy: {(100*test_accuracy):0.2f}%\")\n",
    "\n",
    "        h['train_loss'].append(train_loss)\n",
    "        h['test_loss'].append(test_loss)\n",
    "        h['train_accuracy'].append(train_accuracy)\n",
    "        h['test_accuracy'].append(test_accuracy)\n",
    "\n",
    "    print(f\"Done in {time.time()-start:.3f}secs!\")\n",
    "    \n",
    "    return h"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train the model\n",
    "\n",
    "Let us train for 10 epochs first."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(0)\n",
    "torch.manual_seed(0)\n",
    "\n",
    "device = torch.device('cuda:0')\n",
    "\n",
    "model = Model()\n",
    "print(model)\n",
    "    \n",
    "training_history = trainer(model, train_x, train_y, batch_size=64, epochs=10, \n",
    "                              learning_rate=0.001, device=device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot training history\n",
    "\n",
    "For convenience, we define a function to plot a training history:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a function to plot training history\n",
    "def plot_history(history, figsize=(12, 4)):\n",
    "    plt.close('all')\n",
    "    plt.figure(dpi=100, figsize=figsize)\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(history['train_accuracy'], label='Train')\n",
    "    plt.plot(history['test_accuracy'], label='Test')\n",
    "    plt.legend()\n",
    "    plt.title(\"Accuracy\")\n",
    "    \n",
    "    # plot loss\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(history['train_loss'], label='Train')\n",
    "    plt.plot(history['test_loss'], label='Test')\n",
    "    plt.legend()\n",
    "    plt.title(\"Loss\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, plot the training history of the current model. They will look bizarre at this stage, as explained in the forthcoming section.\n",
    "\n",
    "**Suggested Answer** \n",
    "\n",
    "<details> <summary>Show / Hide</summary> \n",
    "<p>\n",
    "    \n",
    "```python\n",
    "# plot training history\n",
    "plot_history(training_history)\n",
    "```\n",
    "    \n",
    "</p>\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_history(training_history)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Class imbalance\n",
    "\n",
    "In the above history plot, notice how the accuracy of the model converges to a high value very quickly (>90% at the end of the first epoch). Such an odd history indicates that something could be wrong within our dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data distribution\n",
    "\n",
    "Let us inspect the distribution of the data using `plt.hist(train_y)`, paying special attention to the validation part (the final 20%)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot distribution of data\n",
    "plt.figure(dpi=100)\n",
    "plt.hist(train_y, label='Whole dataset')\n",
    "plt.hist(train_y[-len(train_y)//5:], label='Validation subset')\n",
    "plt.xticks([0, 1], ['0: no Ag', '1: with Ag'])\n",
    "plt.xlabel('label')\n",
    "plt.ylabel('number of data')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The histograms show that our dataset is dominated by samples labelled 0 or \"no Ag\", which account for over 95% of the data. Thus, if the model simply learns to *guess* \"no Ag\" in every sample, it can achieve 95% accuracy without learning anything meaningful. This problem is known as **class imbalance**.\n",
    "\n",
    "To avoid this, we must balance the classes. There are a number of strategies we can take:\n",
    "\n",
    "* Upsample the minority class;\n",
    "* Downsample the majority class;\n",
    "* Change the performance metric.\n",
    "\n",
    "The best available option for our problem is to downsample the majority class, which can be easily achieved with `numpy`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# find original indices of 0 ('no Ag') and 1 ('with Ag')\n",
    "id_no_Ag = np.where(train_y == 0)[0]\n",
    "id_with_Ag = np.where(train_y == 1)[0]\n",
    "\n",
    "# downsample 'no Ag' to the number of 'with Ag' by np.random.choice\n",
    "id_no_Ag_downsample = np.random.choice(id_no_Ag, len(id_with_Ag))\n",
    "\n",
    "# concatenate 'with Ag' and downsampled 'no Ag'\n",
    "id_downsample = np.concatenate((id_with_Ag, id_no_Ag_downsample))\n",
    "\n",
    "# shuffle the indices because they are ordered after concatenation\n",
    "np.random.shuffle(id_downsample)\n",
    "\n",
    "# finally get the balanced data\n",
    "train_x_balanced = train_x[id_downsample]\n",
    "train_y_balanced = train_y[id_downsample]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Re-exam the histograms of the balanced dataset after downsampling the majority:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot distribution of downsampled data\n",
    "plt.figure(dpi=100)\n",
    "plt.hist(train_y_balanced, label='Whole dataset')\n",
    "plt.hist(train_y_balanced[-len(train_y_balanced)//5:], label='Validation subset')\n",
    "plt.xticks([0, 1], ['0: no Ag', '1: with Ag'])\n",
    "plt.xlabel('label')\n",
    "plt.ylabel('number of data')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Re-train the model\n",
    "\n",
    "Now we can re-train the model with the balanced dataset. Simply change `train_x` and `train_y` to `train_x_balanced` and `train_y_balanced` in `trainer()` and repeat all the steps in [1. Try out a network](#1.-Try-out-a-network). Note that, to avoid the influence of the initial model state (weights and biases) left by the previous training (such as the one trained with the imbalanced dataset), we have to first re-define the model and reset the `optimizer` before calling `train` step. A larger `epochs` can be used because we now have much fewer data. \n",
    "\n",
    "\n",
    "**Suggested Answer** \n",
    "\n",
    "<details> <summary>Show / Hide</summary> \n",
    "<p>\n",
    "    \n",
    "```python\n",
    "\n",
    "# re-train the model\n",
    "\n",
    "training_history_balanced = trainer(train_x_balanced, train_y_balanced, batch_size=256, epochs=500, device=device)\n",
    "\n",
    "# plot training history\n",
    "plot_history(training_history_balanced)\n",
    "```\n",
    "    \n",
    "</p>\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "np.random.seed(10)\n",
    "torch.manual_seed(10)\n",
    "\n",
    "model = Model()\n",
    "    \n",
    "training_history_balanced = trainer(model, train_x_balanced, train_y_balanced, batch_size=512, epochs=100, \n",
    "                                    learning_rate=0.001, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot training history\n",
    "plot_history(training_history_balanced)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Early stopping\n",
    "\n",
    "The droput has brought the training and validation losses closer to one another. However we also see that there are large oscillations in the validation performance. This is realted to the rather small training and validation set sizes. How can we make sure that we recover the model with the best performance on validation?\n",
    "\n",
    "\n",
    "We can create custom `EarlyStopping` class to track the validation loss over training epochs and save the model every time there is a new best validation loss. We can then also tell our `trainer` to cease if the validation loss has not improved for some steps or epochs; \n",
    "\n",
    "**Suggested Answer** \n",
    "\n",
    "<details> <summary>Show / Hide</summary> \n",
    "<p>\n",
    "    \n",
    "```python\n",
    "class EarlyStopping:\n",
    "    \"\"\"Early stops the training if validation loss doesn't improve after a given patience.\"\"\"\n",
    "    def __init__(self, patience=7, verbose=False, delta=0, path='checkpoint.pt', trace_func=print):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            patience (int): How long to wait after last time validation loss improved.\n",
    "                            Default: 7\n",
    "            verbose (bool): If True, prints a message for each validation loss improvement. \n",
    "                            Default: False\n",
    "            delta (float): Minimum change in the monitored quantity to qualify as an improvement.\n",
    "                            Default: 0\n",
    "            path (str): Path for the checkpoint to be saved to.\n",
    "                            Default: 'checkpoint.pt'\n",
    "            trace_func (function): trace print function.\n",
    "                            Default: print            \n",
    "        \"\"\"\n",
    "        self.patience = patience\n",
    "        self.verbose = verbose\n",
    "        self.counter = 0\n",
    "        self.best_score = None\n",
    "        self.early_stop = False\n",
    "        self.val_loss_min = np.Inf\n",
    "        self.delta = delta\n",
    "        self.path = path\n",
    "        self.trace_func = trace_func\n",
    "    def __call__(self, val_loss, model):\n",
    "\n",
    "        score = -val_loss\n",
    "\n",
    "        if self.best_score is None:\n",
    "            self.best_score = score\n",
    "            self.save_checkpoint(val_loss, model)\n",
    "        elif score < self.best_score + self.delta:\n",
    "            self.counter += 1\n",
    "            self.trace_func(f'EarlyStopping counter: {self.counter} out of {self.patience}')\n",
    "            if self.counter >= self.patience:\n",
    "                self.early_stop = True\n",
    "        else:\n",
    "            self.best_score = score\n",
    "            self.save_checkpoint(val_loss, model)\n",
    "            self.counter = 0\n",
    "\n",
    "    def save_checkpoint(self, val_loss, model):\n",
    "        '''Saves model when validation loss decrease.'''\n",
    "        if self.verbose:\n",
    "            self.trace_func(f'Validation loss decreased ({self.val_loss_min:.6f} --> {val_loss:.6f}).  Saving model ...')\n",
    "        torch.save(model.state_dict(), self.path)\n",
    "        self.val_loss_min = val_loss\n",
    "    \n",
    "def trainer(...):\n",
    "    \n",
    "    ....\n",
    "\n",
    "    early_stopping = EarlyStopping(patience=5)\n",
    "    for t in range(epochs):\n",
    "    \n",
    "        ....\n",
    "        \n",
    "        early_stopping(test_loss, model)\n",
    "        if early_stopping.early_stop:\n",
    "            break\n",
    "    \n",
    "```\n",
    "    \n",
    "</p>\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercises\n",
    "\n",
    "Build a DNN to detect the presence of all the elements. To do this, you may go through the following steps:\n",
    "\n",
    "1. Find all the elements appearing in the dataset; the answer will be `['Zn', 'Sb', 'Si', 'Fe', 'Ag', 'Cu', 'Bi']`.\n",
    "2. Balance the dataset: if one of the elements appears much less times than the others, it is better to ignore it. Doing everything correctly, you will find the number of samples containing each element as shown in the following table. Therefore, we may ignore Ag in this network.\n",
    "\n",
    "\n",
    "|  Element | # Samples |\n",
    "|---|---|\n",
    "|Zn| 51174|  \n",
    "|Sb| 51132|  \n",
    "|Si| 50909| \n",
    "|Fe| 50764|\n",
    "|Ag| 10000|\n",
    "|Cu| 50945|\n",
    "|Bi| 50784|\n",
    "    \n",
    "3. Do one-hot encoding for the element list `['Zn', 'Sb', 'Si', 'Fe', 'Cu', 'Bi']`; if a sample contains Fe and Sb, e.g., the one-hot vector for this sample will be `[0, 1, 0, 1, 0, 0]`.\n",
    "4. Build and train a DNN (with an output size of 6) to detect the presence of the six elements.\n",
    "\n",
    "If doing everything correctly, you will find that the overall accuracy is around 60%. However, the model is not garbage. If we evaluate the accuracy for each element, we will find that the accracy for some of elements is nearly 0 while for the others nearly 100%. This means the dataset is agnostic to these elements, which lower the overall accuracy, but the model can still be used to predict the other elements with  high accuracy.\n",
    "\n",
    "**Suggested Answer** \n",
    "\n",
    "<details> <summary>Show / Hide</summary> \n",
    "<p>\n",
    "    \n",
    "```python\n",
    "##################\n",
    "###### data ######\n",
    "##################\n",
    "element_list = ['Zn', 'Sb', 'Si', 'Fe', 'Cu', 'Bi']\n",
    "train_y = []\n",
    "for element in element_list:\n",
    "    train_y.append(np.array([element in elements for elements in df['Elements']]).astype(int))\n",
    "train_y = np.transpose(np.array(train_y))\n",
    "\n",
    "# print data shapes\n",
    "print(\"Shape of input: %s\" % str(train_x.shape))\n",
    "print(\"Shape of output: %s\" % str(train_y.shape))\n",
    "\n",
    "# create model\n",
    "model = Model(output_dim=6)\n",
    "\n",
    "# train the model\n",
    "training_history = trainer(model, train_x, train_y, batch_size=256, epochs=100, \n",
    "                           learning_rate=0.001, device=device)\n",
    "# plot training history\n",
    "plot_history(training_history)\n",
    "\n",
    "\n",
    "#####################\n",
    "###### predict ######\n",
    "#####################\n",
    "train_x_tensor = torch.from_numpy(train_x).float().to(device)\n",
    "train_y_tensor = torch.from_numpy(train_y).float().to(device)\n",
    "\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    pred_y = model(train_x_tensor)\n",
    "\n",
    "# show overall accuracy and accuracy for each element\n",
    "print(f'Overall accuracy = {binary_accuracy(pred_y, train_y_tensor)}')\n",
    "\n",
    "avg_acc = []\n",
    "for i, element in enumerate(element_list):\n",
    "    print(f'Accuracy for {element} = {binary_accuracy(pred_y[:, i:i+1], train_y_tensor[:, i:i+1])}') \n",
    "```\n",
    "    \n",
    "</p>\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sciml-workshop",
   "language": "python",
   "name": "sciml-workshop"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
