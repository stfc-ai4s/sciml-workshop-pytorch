{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DNN: the Basics\n",
    "\n",
    "In this notebook, we will learn the basics of a Deep Neural Network (DNN) based on [Pytorch](https://pytorch.org/), a Python based machine learning framework for building and training deep neural networks with strong GPU acceleration. \n",
    "\n",
    "We will use the `FashionMNIST` dataset, which is useful for quick examples when learning the basics. In [DNN_practical.ipynb](DNN_practical.ipynb), we will follow the same principles to practice with a more relevant scientific dataset. To understand how a DNN works, we will implement a fully connected DNN from scratch in [DNN_backprop.ipynb](DNN_backprop.ipynb), "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import Pytorch libraries\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision.datasets import FashionMNIST\n",
    "from torchvision.transforms import ToTensor, Compose\n",
    "\n",
    "# check version\n",
    "print('Using PyTorch v%s' % torch.__version__)\n",
    "\n",
    "# helpers\n",
    "import time\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "plt.style.use('ggplot')\n",
    "\n",
    "# for reproducibility\n",
    "torch.manual_seed(0)\n",
    "np.random.seed(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The Dataset and Dataloader\n",
    "\n",
    "PyTorch has two primitives to work with data: torch.utils.data.DataLoader and torch.utils.data.Dataset. Dataset stores the samples and their corresponding labels, and DataLoader wraps an iterable around the Dataset from which samples are loaded. Dataloader supports automatic batching, sampling, shuffling and multiprocess data loading.\n",
    "\n",
    "To start with, we will use the `FashionMNIST` dataset from TorchVision. Every TorchVision Dataset includes two arguments: `transform` and `target_transform` to modify the samples and labels respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download training data from open datasets.\n",
    "training_data = FashionMNIST(\n",
    "    root=\"data\",\n",
    "    train=True,\n",
    "    download=True,\n",
    "    transform=ToTensor(),\n",
    ")\n",
    "\n",
    "# Download test data from open datasets.\n",
    "test_data = FashionMNIST(\n",
    "    root=\"data\",\n",
    "    train=False,\n",
    "    download=True,\n",
    "    transform=ToTensor(),\n",
    ")\n",
    "\n",
    "# print info\n",
    "print(\"Number of training samples: %d\" % len(training_data))\n",
    "print(\"Number of test samples: %d\" % len(test_data))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we define a batch size of 64, i.e. each element in the dataloader iterable will return a batch of 64 features and labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 64\n",
    "\n",
    "# Create data loaders.\n",
    "train_dataloader = DataLoader(training_data, batch_size=batch_size)\n",
    "test_dataloader = DataLoader(test_data, batch_size=batch_size)\n",
    "\n",
    "for X, y in test_dataloader:\n",
    "    print(f\"Shape of X [N, C, H, W]: {X.shape}\")\n",
    "    print(f\"Shape of y: {y.shape} {y.dtype}\")\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can randomly plot some images and their labels:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "string_labels = {\n",
    "    0: \"T-Shirt\",\n",
    "    1: \"Trouser\",\n",
    "    2: \"Pullover\",\n",
    "    3: \"Dress\",\n",
    "    4: \"Coat\",\n",
    "    5: \"Sandal\",\n",
    "    6: \"Shirt\",\n",
    "    7: \"Sneaker\",\n",
    "    8: \"Bag\",\n",
    "    9: \"Ankle Boot\",\n",
    "}\n",
    "\n",
    "# function to plot an image in a subplot\n",
    "def subplot_image(image, label, nrows=1, ncols=1, iplot=0, label2='', label2_color='r'):\n",
    "    plt.subplot(nrows, ncols, iplot + 1)\n",
    "    plt.imshow(image, cmap=plt.cm.binary)\n",
    "    plt.xlabel(label, c='k', fontsize=12)\n",
    "    plt.title(label2, c=label2_color, fontsize=12, y=-0.33)\n",
    "    plt.xticks([])\n",
    "    plt.yticks([])\n",
    "    \n",
    "ncols, nrows = 3, 3\n",
    "plt.figure(dpi=100, figsize=(ncols * 2, nrows * 2.2))\n",
    "\n",
    "for iplot, idata in enumerate(np.random.choice(len(training_data), nrows * ncols)):\n",
    "    img, label = training_data[iplot]\n",
    "    label_str = \"%d: %s\" % (label, string_labels[label])\n",
    "    subplot_image(img.squeeze(0), label_str, nrows, ncols, iplot)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classification by DNN\n",
    "\n",
    "Here we will create and train a DNN model to classify the images in `FashionMNIST`. The task can be divided into three essential steps:\n",
    "\n",
    "1. Build the neural network;\n",
    "2. Optimize the network;\n",
    "\n",
    "These steps may be repeated for a few times to improve the quality (accuracy and stability) of the model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Build the network architecture\n",
    "\n",
    "Our first DNN will be a simple multi-layer perceptron with only one hidden layer, as shown in the following figure:\n",
    "\n",
    "\n",
    "![dense.jpeg](https://github.com/stfc-sciml/sciml-workshop/blob/master/course_3.0_with_solutions/markdown_pic/dnn.png?raw=1)\n",
    "\n",
    "In general, a network of this kind should include an input layer, some hidden layers and an output layer. In this example, all the layers will be `Linear` layers with or without activation function. The linear layer is composed of neurons `W` and biases `b` (optional) and perform linear transformation on the input $x$, i.e. $y=xW^T+b$.\n",
    "\n",
    "\n",
    "### The input layer\n",
    "\n",
    "We first need to determine the dimensionality of the input layer. Since we are using `Linear` layers, we will flatten the input images before feeding them to the network. As the images are 28 $\\times$ 28 in pixels, the flattened input size will be 784. The flattened input will be then passed to the input layer.\n",
    "\n",
    "### The hidden layers\n",
    "\n",
    "We use one hidden layer in this case and use `ReLU` as its activation function:\n",
    "\n",
    "> $R(x)=\\max(0,x)$\n",
    "\n",
    "**NOTE**: Different activation functions are used for different tasks. Remember that `ReLU` generally performs well for training a network, but it can *only* be used in the input and hidden layers.\n",
    "\n",
    "\n",
    "### The output layer\n",
    "\n",
    "We usually encode categorical data as a \"one-hot\" vector. In this case, we have a vector of length 10 on the output side, where each element corresponds to a class of apparel. Ideally, we hope the values to be either 1 or 0, with 1 for the correct class and 0 for the others, so we use `sigmoid` as the activation function for the output layer:\n",
    "\n",
    "> $S(x) = \\dfrac{1}{1 + e^{-x}}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build a neural network in Pytorch\n",
    "class NeuralNetwork(nn.Module):\n",
    "    def __init__(self, input_dim=784, hidden_dim=512, output_dim=10, dropout=0.0):\n",
    "        super().__init__()\n",
    "\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.dropout = dropout\n",
    "        \n",
    "        self.flatten = nn.Flatten()\n",
    "        \n",
    "        self.input_layer = nn.Sequential(\n",
    "            nn.Linear(input_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "        self.hidden_layer = nn.Sequential(\n",
    "            nn.Linear(hidden_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout)\n",
    "        )\n",
    "        self.output_layer =  nn.Sequential(\n",
    "            nn.Linear(hidden_dim, output_dim),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        y = self.flatten(x)\n",
    "        y = self.input_layer(y)\n",
    "        y = self.hidden_layer(y)\n",
    "        y = self.output_layer(y)\n",
    "        return y\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "model = NeuralNetwork(hidden_dim=256, dropout=0.0).to(device)\n",
    "print(model)\n",
    "\n",
    "# perform q quick check using a dummy input\n",
    "inputs = torch.rand((3, 28, 28), device=device) # dummy image with batch size of 3\n",
    "print('Inputs:', inputs.size())\n",
    "outputs = model(inputs)\n",
    "print('Outputs:', outputs.size())\n",
    "\n",
    "y_pred = outputs.argmax(1)\n",
    "print(f\"Predicted class: {[string_labels[idx.item()] for idx in y_pred]}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To define a neural network in PyTorch, we create a class `NeuralNetwork` that inherits from Pytorch `nn.Module` which has functions implementing automatic differentiation. We define the layers of the network in the `__init__` function and specify how data will pass through the network in the `forward` function. \n",
    "\n",
    "In the above network, first, we define a flatten layer with `nn.Flatten` module to converts the 2D input image to 1D contigious array. Note that the mini batch dimension (at dim=0) will be maintained by the operation. \n",
    "\n",
    "Similarly, we define our input, hidden and output layers. Neurons in each layer is realized using the `nn.Linear` module. We use `nn.ReLU` as activation function in the input and hidden layer, whereas `nn.Sigmoid` is used in the output layer. The module `nn.Sequential` allows us to put together the list of sub-modules e.g. the `nn.Layer` and `nn.ReLU` and pass the data through the modules in the same order as defined. We also add `Dropout` layer in the `input` and `hidden` layer for network regularization. \n",
    "\n",
    "Finally, to test the model, we define a dummy input of batch size 3 and place it in the same device as the network (Pytorch require the data and network to be resided on the same device). We pass the data by calling it with the input; `model(inputs)`. This executes the modelâ€™s forward, along with some background operations.\n",
    "\n",
    "Calling the model on the input returns a 2-dimensional tensor with `dim=0` corresponding to each output of 10 raw predicted values for each class, and dim=1 corresponding to the individual values of each output. An `argmax` operation is performed along class label dimension to estimate the class label. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Optimize the network\n",
    "\n",
    "To optimize the neural network, we will need a [loss function](https://pytorch.org/docs/stable/nn.html#loss-functions) and an [optimizer](https://pytorch.org/docs/stable/optim.html) \n",
    "\n",
    "### Loss\n",
    "\n",
    "The loss is the objective function to be minimised during training. Gradients of the loss with respect to the model parameters, calculated by backpropagating the errors, determine the direction to update the model parameters. Follow [DNN_backprop.ipynb](DNN_backprop.ipynb) to learn the details about *backpropagation* and *gradient descent*. In this case, we will use `CrossEntropyLoss` as the loss function because we are dealing with 10 classes.  \n",
    "\n",
    "### Optimizer\n",
    "\n",
    "An optimiser is an algorithm determining how the model parameters are updated based on the loss. One critical hyperparameter for the optimiser is the *learning rate*, which determines the magnitude to update the model parameters (also see [DNN_backprop.ipynb](DNN_backprop.ipynb) for details). In many applications, *Adam* is usually a good choice at the beginning. We will use Adam in this example.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define loss function\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "# create a optimizer with constant learning rate of 0.001\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To create a loss function, we will use the modules provided inside [torch.nn](https://pytorch.org/docs/stable/nn.html) and initialize an instance of `CrossEntropyLoss`. Similarly, PyTorch provides a wide range of optimizers (see [torch.optim](https://pytorch.org/docs/stable/optim.html#torch.optim.Optimizer)). We will use `Adam` optimizer for our example as it works well in most cases. The PyTorch optimizer accepts an iterable containing the parameters (all should be [Variable's](https://pytorch.org/docs/stable/generated/torch.nn.parameter.Parameter.html#torch.nn.parameter.Parameter)) to optimize and the rate at which to update the parameters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Train the Neural Network\n",
    "\n",
    "Now we can start to train the model with `FashionMNIST` dataset. Before that, we need to understand the concepts of `epochs` and `mini-batch size`.\n",
    "\n",
    "### Epochs\n",
    "\n",
    "It is the number of times that the model will run through the entire dataset during training.\n",
    "\n",
    "### Mini-batch size\n",
    "\n",
    "It determines how many data will be used at a time to determine the gradient used for parameter update. Follow [DNN_backprop.ipynb](DNN_backprop.ipynb) to learn more about *Batch*, *Mini-batch* and *Stochastic Gradient Descent*.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training loop\n",
    "Unlike Keras, PyTorch doesn't have a pre-defined training loop, so we will create a simple training loop. To train the neural network, usually, we will iterate over the dataset in mini-batches, pass the mini-batches to the network, calculate the loss, compute gradients by backprogating the errors and update the weights of the neural network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define a train function\n",
    "def train(dataloader, model, loss_fn, optimizer, device):\n",
    "    size = len(dataloader.dataset)\n",
    "    num_batches = len(dataloader)\n",
    "    model.train()\n",
    "    \n",
    "    train_loss, train_accuracy = 0, 0\n",
    "    for batch, (X, y) in enumerate(dataloader):\n",
    "\n",
    "        # place tensors to device\n",
    "        X, y = X.to(device), y.to(device)\n",
    "\n",
    "        # Compute prediction error\n",
    "        pred_y = model(X)\n",
    "\n",
    "        # convert target class indices into one-hot encoding\n",
    "        target_y = F.one_hot(y, 10).to(torch.float32)\n",
    "\n",
    "        # compute loss\n",
    "        loss = loss_fn(pred_y, target_y)\n",
    "\n",
    "        # Backpropagation\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        train_accuracy += (pred_y.argmax(1) == y).type(torch.float).sum().item()\n",
    "        train_loss += loss.detach().item()\n",
    "    \n",
    "    train_loss /= num_batches\n",
    "    train_accuracy /= size\n",
    "    \n",
    "    return train_loss, train_accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Validation loop\n",
    "\n",
    "To assess the network's performances in new and unseen data, we will use the validation dataset at the end of training. However, the validation of the network can be done at the middle of training that helps us identify under-fitting and over-fitting problem. To make predictions with a confidence equivalent to that for training, the accuracy for the validation data should not differ too much from that for the training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define a test function to evaluate the model\n",
    "def test(dataloader, model, loss_fn, device):\n",
    "    # set the model to eval mode\n",
    "    model.eval()\n",
    "    \n",
    "    size = len(dataloader.dataset)\n",
    "    num_batches = len(dataloader)\n",
    "    test_loss, test_accuracy = 0, 0\n",
    "    with torch.no_grad():\n",
    "        for X, y in dataloader:\n",
    "            X, y = X.to(device), y.to(device)\n",
    "            \n",
    "            # inference/prediction\n",
    "            pred_y = model(X)\n",
    "\n",
    "            # convert target class indices into one-hot encoding\n",
    "            target_y = F.one_hot(y, 10).to(torch.float32)\n",
    "\n",
    "            # compute test loss and accuracy\n",
    "            test_loss += loss_fn(pred_y, target_y).item()\n",
    "            test_accuracy += (pred_y.argmax(1) == y).type(torch.float).sum().item()\n",
    "            \n",
    "    test_loss /= num_batches\n",
    "    test_accuracy /= size\n",
    "\n",
    "    return test_loss, test_accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The training process is conducted over several iterations (epochs). During each epoch, entire samples is passed through the network and network's parameters are updated to make better predictions. At the end of each epoch, it is better to compute the accuracy on the test dataset to evaluate the generalizabilty of the network in unseen data. We will create a simple `trainer` function that does this function. We will also record the loss and accuracy obtained during each epoch for analysis and visualizations. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trainer(model, train_dataloader, test_dataloader, optimizer, loss_fn, epochs, device):\n",
    "    start = time.time()\n",
    "    h = dict(train_loss=[], test_loss=[], train_accuracy=[], test_accuracy=[])\n",
    "    for t in range(epochs):\n",
    "        print(f\"\\nEpoch {t+1}/{epochs}: \", end='')\n",
    "        \n",
    "        train_loss, train_accuracy = train(train_dataloader, model, loss_fn, optimizer, device)\n",
    "\n",
    "        test_loss, test_accuracy = test(test_dataloader, model, loss_fn, device)\n",
    "        print(f\"train_loss: {train_loss:0.3f}, train_accuracy: {(100*train_accuracy):0.2f}%, test_accuracy: {(100*test_accuracy):0.2f}%\")\n",
    "\n",
    "        # print(f\"Test Accuracy: {(100*test_accuracy):0.2f}%, Test Loss: {test_loss:0.3f}\")\n",
    "\n",
    "        h['train_loss'].append(train_loss)\n",
    "        h['test_loss'].append(test_loss)\n",
    "        h['train_accuracy'].append(train_accuracy)\n",
    "        h['test_accuracy'].append(test_accuracy)\n",
    "\n",
    "    print(f\"Done in {time.time()-start:.3f}secs!\")\n",
    "    \n",
    "    return h\n",
    "\n",
    "# plot accuracy\n",
    "def plot_history(history, figsize=(12, 4)):\n",
    "    plt.close('all')\n",
    "    plt.figure(dpi=100, figsize=figsize)\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(history['train_accuracy'], label='Train')\n",
    "    plt.plot(history['test_accuracy'], label='Test')\n",
    "    plt.legend()\n",
    "    plt.title(\"Accuracy\")\n",
    "    \n",
    "    # plot loss\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(history['train_loss'], label='Train')\n",
    "    plt.plot(history['test_loss'], label='Test')\n",
    "    plt.legend()\n",
    "    plt.title(\"Loss\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let`s train the network!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 100\n",
    "history = trainer(model, train_dataloader, test_dataloader, optimizer, loss_fn, epochs, device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check training history\n",
    "\n",
    "We can examine the training history by plotting accuracy and loss against epoch for both the training and the test data. \n",
    "\n",
    "Notice that the accuracies for the training and the test data diverge as the model trains (after around epoch 50). This is a classic symptom of [overfitting](https://en.wikipedia.org/wiki/Overfitting), that is, our model corresponds too closely to the training data so that it cannot fit the test data with an equivalent accuracy.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_history(history)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Finding the best epoch for early-stopping\n",
    "The test accuracy at the end of the training may not always represent the optimal value because the network can reach the peak accuracy even before the final epoch. It is good idea to identify such point and stop the training because training beyond this point doesn't improve the test accuracy. This strategy is also known as early-stopping and is widely used to minimize the training time. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Best train accuracy: {np.max(history['train_accuracy']):0.2f} at epoch {np.argmax(history['train_accuracy'])}\")\n",
    "print(f\"Best test accuracy: {np.max(history['test_accuracy']):0.2f} at epoch {np.argmax(history['test_accuracy'])}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperparameter Tunning\n",
    "\n",
    "There are several factors affecting the prediction accuracy e.g. model complexity, learning rate, batch size and so on. We will need to address each factor step-by-step. For instance, we can decrease the model complexity by reducing the `hidden_dim` by half and re-run the experiment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = NeuralNetwork(hidden_dim=128).to(device)\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "history =  trainer(model, train_dataloader, test_dataloader, optimizer, loss_fn, epochs)\n",
    "plot_history(history)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Regularisation\n",
    " \n",
    "Regularisation of the network helps to mitigate the overfitting and gradient exploding problem. Dropout, also called dilution, is a popular regularisation technique which randomly omit a certain amount of neurons from a layer with probability `p`. Here we will rebuild our model with `Dropout` layer between the hidden and the output layers. Let us see whether this can negate the overfitting or not. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 256\n",
    "# Create data loaders.\n",
    "train_dataloader = DataLoader(training_data, batch_size=batch_size)\n",
    "test_dataloader = DataLoader(test_data, batch_size=batch_size)\n",
    "\n",
    "model = NeuralNetwork(hidden_dim=128, dropout=0.2).to(device)\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=2e-3)\n",
    "history =  trainer(model, train_dataloader, test_dataloader, optimizer, loss_fn, epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_history(history)\n",
    "print(f\"Best train accuracy: {np.max(history['train_accuracy']):0.2f}\")\n",
    "print(f\"Best test accuracy: {np.max(history['test_accuracy']):0.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Make predictions/inference\n",
    "\n",
    "Finally, we can use our trained model to make predictions. Here we show some wrong predictions for the test data, from which we may get some ideas about what kinds of images baffle our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use test images to make predictions\n",
    "def predict(dataloader, model):\n",
    "    # set the model to eval mode\n",
    "    model.eval()\n",
    "    \n",
    "    gt_images, gt_labels, pred_labels = [], [], []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for X, y in dataloader:\n",
    "            \n",
    "            # inference/prediction\n",
    "            pred_y = model(X.to(device)).cpu().numpy()\n",
    "\n",
    "            gt_images.append(X.numpy())\n",
    "            gt_labels.append(y.numpy())\n",
    "            pred_labels.append(pred_y.argmax(1))\n",
    "\n",
    "    gt_images = np.concatenate(gt_images)\n",
    "    gt_labels = np.concatenate(gt_labels)\n",
    "    pred_labels = np.concatenate(pred_labels)\n",
    "    \n",
    "    return gt_images, gt_labels, pred_labels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataloader = DataLoader(test_data, batch_size=1024)\n",
    "\n",
    "test_images, test_labels, pred_labels = predict(test_dataloader, model)\n",
    "\n",
    "# convert to numpy\n",
    "test_images = test_images.cpu().numpy()\n",
    "pred_labels = pred_labels.cpu().numpy()\n",
    "test_labels = test_labels.cpu().numpy()\n",
    "\n",
    "# get the indices of wrong predictions\n",
    "id_wrong = np.where(pred_labels != test_labels)[0]\n",
    "print(\"Number of test data: %d\" % test_labels.size)\n",
    "print(\"Number of wrong predictions: %d\" % id_wrong.size)\n",
    "\n",
    "# plot the wrong predictions\n",
    "nrows = 4\n",
    "ncols = 8\n",
    "plt.figure(dpi=100, figsize=(ncols * 2, nrows * 2.2))\n",
    "for iplot, idata in enumerate(np.random.choice(id_wrong, nrows * ncols)):\n",
    "    label = \"%d: %s\" % (test_labels[idata], string_labels[test_labels[idata]])\n",
    "    label2 = \"%d: %s\" % (pred_labels[idata], string_labels[pred_labels[idata]])\n",
    "    subplot_image(test_images[idata].squeeze(), label, nrows, ncols, iplot, label2, 'r')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercises\n",
    "\n",
    "* Change some hyperparameters like `hidden_dim`, `dropout` in the `NeuralNetwork` to see their effects.  \n",
    "* Use two hidden layers respectively with sizes 256 and 64, and see whether the accuracy can be improved or not;\n",
    "* Change the output from 0-1 binary to probability, i.e., the one-hot vector represents the probabilities that an image belongs to the classes; this can be achieved by 1) replacing the `nn.Sigmoid()` in the output layer with a `nn.Softmax()`.\n",
    "* Double the batch size and learning rate\n",
    "* Change the optimizer to `AdamW` (see other [optmizers](https://pytorch.org/docs/stable/optim.html)) with or without weight regularization "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:sciml-workshop] *",
   "language": "python",
   "name": "conda-env-sciml-workshop-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
